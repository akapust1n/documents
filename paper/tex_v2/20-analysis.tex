\section*{Методы поиска аномалий}
\label{cha:analysis}
\textbf{Аннотация}: В работе проводится сравнительный анализ существующих методов поиска аномалий на неразмеченных наборах данных. Приводится описание методов улучшения   существующих алгоритмов. Предложен способ усовершенствования существующих алгоритмов путем их ансамблирования.

\textbf{Ключевые слова}: Аномалия, поиск аномалий, машинное обучение.
\section{Введение} 
Задача поиска аномалий является одной из классических задач машинного обучения. В настоящее время задачу поиска аномалий активно решают во многих областях жизнедеятельности:
\begin{enumerate} 
	\item Защита информации и безопастность
	\item Социальная сфера и медицина
	\item Банковская и финансовая отрасль
	\item Распознавание и обработка текста, изображений, речи
	\item Другие сферы деятельности(например, мониторинг неисправностей механизмов)
\end{enumerate} 

Количество данных в мире удваивается примерно каждые два года. Поэтому актуальной задачей является разработка новых методов и усовершенствование старых методов поиска выбросов.

\section{Классификация методов обнаружений аномалий}
Классическая система классификации предполагает предварительное обучение на обучающем наборе данных и последующую классификацию на основе этого набора. Данные делятся на "обучающую выборку"\ - данные, при помощи которых алогритм обучает классификар и, "тестовую выборку"\ - данные, при анилизе которых классификатор остается неизменным. Тестовая выборка нужна для того чтобы проверить корректность обучения классификатора.

 Однако, в случае с поиском аномалий, возможны варианты, отличающиеся от классического. Подходящий метод классификации выбирается на основе наличия разметки данных.   Выделяются три основых класса методов:
\begin{enumerate}
\item Обучение с учителем. Для обучения необходимо наличие полностью  размеченных данные для обучения и для тестов. Классификатор  обучается один раз и применяться впоследствии.В связи с тем, что для многих наборов данных заранее неизвестно что является аномалией, а что нет, применение этого метода ограничено.
\item Обучение с частичным привлечением учителя. Для обучения необходимо наличие тествого и учебного набора данных. Однако, в отличие от обучения с привелечением учителя, разметка данных не требуется. Все данные, представленные в выборках, считаются нормальными. На основе этих данных строится некая модель. Все данные, отклоняющиеся от этой модели, считаются аномальными.
\item Обучение без учителя.
Самый гибкий способ, который не требует разметки набора данных.  Идея заключается в том, что алгоритм обнаружения аномалий оценивает данные исключительно на основе внутренних свойств набора данных что является нормальным, а что является выбросом. В данной работе основное внимание будет этому  именно этому способу. 
\end{enumerate}

\section*{Результат метода обнаружения аномалий}
В результате работы алгоритма обнаружения аномалий  с элементом данных связывается  метка или оценка достоверности(показатель аномальности).  Метка - показатель, который принимает нулевое значения, в случае если она связана с нормальными данными и единицу в противном случае. Оценка показывает вероятность того, что элемент является аномалией. Для разных алгоритмов используется разные шкалы оценок, поэтому приведение конкретных примеров оценок будет некорректным.  В алгоритмах метода обучения с учителем зачастую используются метки как выходные данные, в  алгоритмах  с частичным привлечением учителя и без учителя  обнаружения аномалий чаще встречаются оценки.


\section*{Вероятностно-генеративные методы}
Основная идея генеративных методов заключается в использование вероятностного смесевого моделирования данных. Предлагается подобрать такую вероятностую модель, из которой было получены нормированные данные. Такие модели обычно называются генеративными моделями, где для каждой точки(элемента данных) можем посчитать генеративную вероятность(или вероятность правдоподобия).Т.е. задача  сводится к нахождению плотности распределения p(x). Аномалиями при этом  считаются точки(элементы набора данных), имеющию низкое правдоподобие. В качестве показателя аномальности выступает функция p.
Для построения генеративной модели нужно решить следующую задачу:
	\begingroup
	\Large
	\begin{equation*}
	\prod \limits_{x \in X_{norm}} p(x,\theta)  \rightarrow max_\theta
		\end{equation*}
	\endgroup
		где \begingroup \Large$ X_{norm}$ \endgroup - нормальные данные представленного набора данных ${p(x,\theta)|\theta \in \omega}$ -семейство плотностей вероятностей, параметризованные $\theta$.
		
Этот метод редко используется на практике, так как тяжело проверить полученную генеративную модель на адекватность, сложно  убедится в правильном выборе семейства смесевых распределений. Это связано с тем, что низкое значение функции правдоподобия может означать как и аномальное значение, так и неудачно подобранную модель. Этот метод применяется с опорой на априорную информацию, в случае когда можно проверить полученную модель на адекватность.
\section*{Линейные методы}
Основной идеей линейных методов является построение некой  модели, характеризующей нормальные данные. Точки, которые значительно отклоняются от этой модели, считаются аномалиями.

Предполагается, что нормальные данные  находятся в подпрострастрансве пространства атрибутов данных(размер подпространства атрибутов данных равен размерности данных). В свою очередь, задача линейного метода - найти низкоразмерные подпространства, такие что, выборка данных этого подпространства значительно отличается от остальных точек пространства данных.

Одним из возможных вариантов решения является использование линейной регрессии. Выбирается одна из наблюдаемых переменных  набора данных и относительно неё решается задача линейной регрессии оставшихся атрибутов. Итоговым ответом будет является усредненное значения показателя аномалии по всем атрибутам. 

Алгоритмы, основанные на линейном подходе, требуют  наличия линейной зависимости атрибутов данных. 
\section*{Метрические методы}
Мерические методы пытаются найти в данных точки, в некотором смысле
изолированные от остальных[ссылка на источник]. Если в пространстве задана некоторая метрика \textit{p(x1, x2)}, то необходимо задать следующие понятия:
\begin{itemize}
	\item  Аномалии – точки, не попадающие ни в один кластер. К данным применяется один из алгоритмов кластеризации; размер кластера, в котором оказалась точка, объявляется её показатель аномальности.
	\item Локальная плотность в аномальных точках низкая. Для данной точки показателем аномальности объявляется локальная плотность, которая оценивается некоторым непараметрическим способом.
	\item Расстояние от данной точки до ближайших соседей велико.
\end{itemize}
 В качестве показателя аномальности может выступать:
 \begin{itemize}
\item расстояние до k-го ближайшего соседа;
\item среднее расстояние до k ближайших соседей;
\item медиана расстояний до k ближайших соседей;
\item гармоническое среднее до k ближайших соседей;
\item доля из k ближайших соседей, для которых данная точка является не
более чем k-ым соседом и многое другое.
\end{itemize}

Метрические методы используют в случае отсутсвия априорной информации о данных. Сложность вычисления прямо пропорциональна как размерности данных m, так и их количеству n. При росте набора данных наблюдается экспоненциальный рост сложности вычислений. Однако, эти методы хорошо проявляют себя на ограниченных наборах данных[ссылка на источник]. Следовательно такие методы как k-ближайших соседей) с нотацией ассимптотического роста $O(n^2)$ недопустимы для наборов данных с большой размерностью, если их размерность не может быть уменьшена.



\section*{Методы улучшения алгоритмов поиска аномалий}
Алгоритмы поиска аномалий можно улучшать различными методами, применяемыми в том числе для  улучшения результатов работы алгоритмов и в других областях. Например, ансамблирование широко применяется для работы с нейронными сетями. Ниже рассмотрены приемы в контексте поиска аномалий.
\subsection*{Cемплирование}
Большинство алгоритмов распознавания аномалий успешно работают на наборах данных малых размеров. Поэтому предлагается разбить начальный набор данных на несколько случайных выборок и усреднить результат. Размер этих выборок может быть как и случайным, там и фиксированного размера, но, как правило, он отличается от размеров исходного набора данных не меньше чем на порядок. Идея такого выбора заключается в том, что шумовые объекты попадут в выборки с низкой вероятностью; кластера нормальных данных будут представлены несколькими представителями, а кластера аномалий выродятся в изолированные точки. На основе этих выборок алгоритмы строят функции показателя аномальности, незначительно уступующему результату, полученному на основе анализа всех исходных данных. 

Этот метод помогает значительно сократить вычислительную сложность, а так же уменьшить вероятность "подгона"\ алгоритма под конкретный набор данных.  В силу особенностей задачи, необходимое условие - отсутствия параметризации алгоритмов -  зачастую означает их детерминированность(в отсутсвии стохастичности, показатель аномальности однозначно определяется по  заданной выборке). В общем случае при добавлении новых данных в общий набор данных, можно не пересчитывать заново показатель аномальности для всего набора данных, а добавить запуски алгоритма на новых данных в ансамбль(так называемый warm start[ссылка на источник])
\subsection*{Ансамблирование голосованием}
Ансамблированием в задаче поиска аномалий называют использование нескольких различных алгоритмов с последующи усреднением их показателя аномальности. При использовании различных классов алгоритмов можно столкнуться с проблемой того, что показатель аномальности выглядит по-разному в различных алгоритмах и сравнивать напрямую эти показатели некорректно.  Поэтому традиционное приведение  показателей значений различных функций к одному диапазону, например,к [0,1], будет некорректным.
Существует несколько наиболее известных видов ансамблирования:
\begin{itemize}
	\item Простое голосование 
	\begin{equation*}
	b(x)=F(b_1(x),...,b_T(x))=\frac{1}{T}\sum_{t=1}^{T}b_t(x)
	\end{equation*}
	,где $b_i$ -  некоторая функция.
	\item Взвешенное голосование 
	\begin{gather*}
	b(x)=F(b_1(x),...,b_T(x))=\frac{1}{T}\sum_{t=1}^{T}w_tb_t(x)\\
		\sum_{t=1}^{T}w_t=1, w_t \geq 0
	\end{gather*}
	,где $w_i$- некоторый коэффициент.
		\item Cмесь экспертов
		\begin{gather*}
		b(x)=F(b_1(x),...,b_T(x))=\frac{1}{T}\sum_{t=1}^{T}w_t(x)b_t(x)\\
		\sum_{t=1}^{T}w_t=1, \forall x\in X
		\end{gather*}
		,где $w_i(x)$- некоторая функция коэффициента.
 \end{itemize}
	Простое голосование - это  частный случай взвешенного голосования, а взвешенное голосование является частным случаем смеси экспертов. 
	
	Различные методы ансамблирования такие как беггинг, бустинг, стекинг и другие применяются для улучшения работы алгоритмов обучения с учителем . Для алгоритмов обучения без учителя применяется простое голосование, т.к. задача изменения весов голосования нетривиальна в задаче обучения без учителя[ссылка на источник].
\subsection*{Итеративный отбор}
Итеративный отбор основан на идее многократного применения алгоритмов ансамблирования. Преположим, построена некоторая модель, описывающая нормальные данные. Эта модель построена на основе всех  имеющихся данных, но точность этой модели невелика, она умеет определять только явные аномалии. Отсортировав все точки по показателю аномальности, можно выбрать k самых аномальных объекта в данных и исключить из данных. После этого можно перестроить модель и повторить вышеуказанные действия несколько раз, пока не будут достигнуты некоторые условия. При каждой итерации точность модели будет увеличиваться.

Идея итеративного отбора может быть обобщена различными способами. Результат работы одного алгоритма может быть использован для отсеивания явных аномалий и настройки нового алгоритма, не обязательно совпадающего с предыдущим, на оставшихся данных. Возможна и противоположная механика: по результатам работы одного алгоритма отбираются явные, гарантированные
представители нормальных данных, и исключительно на них строится модель, их описывающая.
\section*{Cравнение методов}

\begin{table*}[!h]
	
	\caption{\label{tab:collectdata}Сравнение алгоритмов поиска аномалий}
	
	\begin{center}
		
		\begin{tabular}{|l|l|l|l|}
			
			\hline
			
			Класс методов & Временная сложность & Расход памяти & Универсальность \\
			
			\hline \hline
			
			Вероятностно-ген.& O(1) &  O(n) & Очень низкая \\
			
			
			\hline
			
			Линейный &  $\ge O(n^2)$ &  $\ge O(n^2)$ & Низкая\\
			
			
			\hline
			Параметрический & O(1) & O(n) & Низкая\\
			\hline
			Метрический & $\ge O(nlogn)$ & $\ge O(n)$ & Высокая\\
			
			
			\hline
			
			
		\end{tabular}
		
	\end{center}
	
\end{table*}                               
Под универсальностью понимается возможность применять алгоритмы к различным набором данных, не обладающих специфическими характеристиками, и, не обладая априорной информацией, получать высокую точность классификации.

\section*{Улучшенный метод поиска аномалий}
Исходя из характеристик вышеописанных методов, можно сделать вывод о том, что метрические методы поиска аномалий обладают наибольшей универсальностью. Также метрические методы легко совмещать за счёт единой методики измерения показателя аномальности.
Можно предложить новый метод поиска аномалий - ансамблирование нескольких метрических методов. Было выбрано три метрических метода - метод K ближайших соседей, метод компонентного коэффициента выбросов[ссылка на литературу по методам].
Для проверки работоспособности алгоритмов поиска аномалий на неразмеченных данных, эти алгоритмы проверялись на размеченных данных.
Для этого работа алгоритма поиска аномалий была протестирована на двух наборах данных[ссылка на описание наборов данных]:
\begin{table*}[!h]
	
	\caption{\label{tab:issled1}Характеристики датасетов, метрики полноты и точности}
	
	\begin{center}
		
		\begin{tabular}{|l|l|l|l|l|l|}
			
			\hline
			
			Набор данных& Кол-во элем. & Кол-во атриб. &  Полнота & Точн.& Кол-во аном.  \\
			
			\hline 
			
			WBC& 453 & 9 & 0.99&0.94 & 10  \\
			
			\hline
			KDDCUP99 & 60853 & 41 & 0.93&0.06 & 246  \\
			\hline
			
			
		\end{tabular}
		
	\end{center}
	
\end{table*}

\begin{table*}[h]
	
	\caption{\label{tab:issled2}Сравнение  алгоритмов поиска аномалий}
	
	\begin{center}
		
		\begin{tabular}{|l|l|l|l|l|}
			
			\hline
			
			Алгоритм & AUC ROC WBC & F1 WBC &  AUC ROC KDD & F1 KDD \\
			
			\hline 
			
			LoOp& 0.98 & 0.72 & 0.68& 0.05  \\
			
			\hline
			ODIN & 0.62 & 0.80 & 0.80& 0.06  \\
			
			\hline 
			KDEOS & 0.25	 & 0.64 & 0.61& 0.05  \\
			
			\hline 
			LDOF & 0.64	 & 0.96 & 0.88&0.07  \\
			\hline 
			INFLO & 0.99	 & 0.9 & 0.98&0.29  \\
			\hline   
			Разр. алгоритм & 0.92	 & 0.97 & 0.93 & 0.06  \\
			
			\hline  
			
		\end{tabular}
		
	\end{center}
	
\end{table*}
\begin{table*}[h]
	
	\caption{\label{tab:issled2}Количество истинно/ложно позитивно/негативно классифицировавшихся}
	
	\begin{center}
		
		\begin{tabular}{|l|l|l|l|l|}
			
			\hline
			
			Набор данных & ИП & ЛП &  ИН & ЛН \\
			
			\hline 
			
			WBC & 10 & 18 & 425 & 0  \\
			\hline 
			
			KDDCUP99& 230 & 3603 & 57004& 16  \\	 
			
			\hline  
			
		\end{tabular}
		
	\end{center}
	
\end{table*}
Проведем сравнения с другими алгоритмами поиска аномалий.
Как можно увидеть из результатов  метрик AUC ROC и F1, алгоритмы по-разному классифицируют  разные наборы данных.Например, алгоритм LoOP показывает высокий AUC ROC на первом наборе данных, но на втором наборе данных его показали значительно снижаются. В свою очередь, алгоритм ODIN показывает низкие результаты, по сравнению с остальными алгоритмами, на первом наборе данных, но на втором наборе данных его AUC ROC высок. Разработанной алгоритм показывает средние значения AUC ROC, но высокие значения показателя F1, что позволяет утверждать, что этот алгоритм жизнеспособен и возможно его применение на определенных наборах данных. 




\section*{Заключение}
Задача поиска аномалий достаточно нетривиальна, а способы её решения могут сильно различаться в зависимости от характектеристик данных и цели поиска. Был предложен новый метод поиска аномалий. Исходя из количества истинно позитивных результатов и ложно позитивных результатов, можно рекомендовать использовать данный метод в задачах где акцент делается на нахождении истинно позитивных значений, пренебрегая некоторым количеством полученных ложно позитивных значений.
