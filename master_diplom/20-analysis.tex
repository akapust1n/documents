\chapter{Аналитический раздел}
\label{cha:analysis}
\section{Цель и задачи работы}
Целью данной работы является создание метода прогнозирования результатов теннисных матчей на основе априорной информации.
Для достижения данной цели необходимо решить следующие задачи:
\begin{itemize}
	
\item проанализировать предметную область и существующие методы прогнозирования результатов теннисных матчей
	\item разработать метод прогнозирования результатов теннисных матчей
	\item создать ПО, обрабатывающее данные для анализа
	\item создать ПО, реализующее  разработанный метод прогнозирования результатов теннисных матчей
\end{itemize}
\section{Что такое спортивное прогнозирование}
Спортивное прогнозирование предполагает предугадывание результатов предстоящих спортивных событий или контрольных результатов, которые спортсмен) или команда спортсменов) показывает на спортивных соревнованиях\cite{Book01}. Прогнозы даются на конкретные события в конкретные моменты времени,на результат совокупности событий,ограниченных во времени(например, соревновательный сезон). Наиболее распространены прогнозы на результаты конкретного матча и сезона в целом. Прогнозы могу осуществляться на основе алгоритмов анализа информации, экспертной оценке, а так же комбинацией экспертной оценки. В данной работе будет рассмотрено прогнозирование на основе анализа априорной информации. Т.е. прогноз на какое-либо событие будет даваться до его начала и текущая информация о ходе события не будет учитываться.
\section{Сложность прогнозирования результатов}
 Большинство исследователей используют различную статистическую информацию для составлении прогнозов. Из массива накопленных данных они выбирают небольшое количество набиболее важных показателей(или же останавливаются только на одном из них) за ограниченный промежуток времени, которые подаются на вход алгоритму.
 Какие показатели считать значитыми определяет автор алгоритма на основе экпертной оценки или каких-то дополнительных алгоритмов "отбора наиболее важных показателей\".
 Например, в случае американского футбола\cite{Book02} это могут быть
 \begin{enumerate}
 	\item  Время владения мячом
 	\item Проводился ли матч дома или в гостях
 	\item Общее количесто ярдов
 	\item Разница в атакующих ярдах
 	\item Количество потерь мяча
 \end{enumerate}
Или же в случае баскетбола, например, может браться следующий набор показателей:
 \begin{enumerate}
	\item  Количество травмированных игроков
	\item Количество выигранных матчей подряд перед данной игро
	\item Усталость команды. Показатель считается на основе расстояния, которое пришлось преодолеть команде, чтобы провести последние 7 игр
	\item Средний домашний, гостей и общий процент побед
	\item Рейтинг команды в "рейтинге нападения", "рейтинге защиты" и "общем рейтинге"(подробнее методики подсчёта рейтингов описываются в \cite{Book03})
\end{enumerate}
Большая часть статей представляет команду как некое единое целое, упуская из виду каждого игрока команды. Учёт информации каждом игроке - нетривиальная задача.
Поэтому для упрощения создания алгоритма прогнозирования с учётом каждого игрока команды был выбран вид спорта - одиночный теннис. В каждой команде по одному игроку, два возможных исхода матча - победа или поражения.
% тут явно можно написать больше про
\section{Основные правила игры в теннис}
Ниже приведнены основные правила, которые могут повлиять на составление алгоритмов прогнозирования. С полной версии правил можно ознакомится на сайте международной теннисной федерации\cite{Book04}.
 \begin{enumerate}
	\item  Мяч должен приземлиться в пределах теннисного поля, чтобы продолжить игру. Если игрок мяч приземляется за пределами поля, то это приводит к потере им очков.
	\item Игроки не могут дотронуться до сетки или перейти на сторону противника
	\item Игроки не могут носить мяч или ловить его ракеткой
	\item Игроки не могут дважду ударить по мячу
    \item Игрок должен дождаться пока мяч пересечёт сетку, прежде чем ударить по нему
    \item Игрок, который не возвращает мяч на половину поля противника, преждне чем он отскочит дважды, считается проигравшим
    \item Если мяч ударяет или касается игроков, то это карается штрафом
    \item Перед тем как отбить подачу, мяч должен отскочить от поля.
    \item Очки -наименьшая единица изменрения.Приращение очков идёт в формате 0-15-30-40-гейм.
    \item Гейм состоит из 4 очков и выигрывается, когда игрок набирает 4 очка с преимуществом не менее двух очков.
    \item Сет состоиз 6 геймов и выигрывается игроком, который набирает 6 геймов 
 	 с минимальным отрывом в 2 очка.
 	 \item Дополнительный сет - сет, который разыгрывается при достижении игроками счёта 6-6 по геймам. При
  	\item Максимальное количество сетов в матче может достигать 3 или 5. По достижении 2 или соотвественно 3 выигранных сетов матч заканчивается.
  	\item "Ровно"\ происходит когда достигнут счёт по очкам 40-40. Чтобы выиграть гейм, игрок должен выиграть два последовательных очках. Если игрок выигрывает одно очко, то счёт в гейме становится "больше"\, но если он теряет следующее очко, то счёт возвращается к "ровно"\ .
  	\item Когда в гейме достигнут счёт 6-6, то играется тай-брейк. В розыгрыше тай-брейке используются особые правила набора очка. Победителей считается игрок, набравший не менее 7 очков с преимуществом два очка над оппонентом.
\end{enumerate}
\section{Основные подходы прогнозирования теннисных матчей}
Существует три основных подхода к прогнозированию теннисных матчей: методы попарного сравнения, методы вычисления очков и методы машинного обучения.
\subsection{Метод подсчёта очков}
Методы вычисления очков(также их называют иерархическими методами) фокусируются на оценке вероятности выигрыша каждого очка в матче. 
% P. K. Newton and J. B. Keller
В этом подходе предполагается, что достаточно знать вероятность выигрыша игроком А $p_a^r$ очка на своей подаче и вероятность выигрыша очка игроком B на своей подаче $p_b^r$. Аналогичные подходы  применяются при подсчете вероятности выигрыша игрока в бадминтоне\cite{Book19} и сквоше\cite{Book20}.Впервые такой метод был применен к теннису в работах Кси и Бюрича\cite{Book21}, Картера и Крю\cite{Book22}, Полларда\cite{Book23}.
Такие работы одинаково определяют вероятность выигрыша очка в матче как равномерную случайную величину, т.е. вероятности  $p_a^r$ и  $p_b^r$ принимаются за постоянные величины на протяжении всего матча. Анализ статистических показателей показывает, что такое предположение допустимо\cite{Book09}, т.к. отклонения от ожидаемой величины невелики.
\subsubsection{Вероятность выиграть гейм}
Игрок А может выиграть гейм у игрока В со счётом [4,0], [4,1],4,2] в случае если игрок А выиграл 4 очка за гейм, а игрок В выиграл не более двух. Если же счёт в гейме стал [3,3], то такая ситуация называется "ровно"\ . В этом случае игрок A может выиграть гейм, закончив его с итоговым счетом [n + 5, n + 3], где $n\geq0$. Чтобы посчитать вероятность $p_G^R$ выигрыша игроком А гейма на своей подаче, введем следующие обозначения: $p_A^R$ - вероятность выигрыша очка на своей подаче игроком А, $q_A^R=1-p_a^R$, $q_A^G = 1 - p_A^G$, $p_A^G(i,j)$ - вероятность того, что i очков в гейме наберет игрок А, а j очков в гейме наберет игрок B. В результате получим следующую формулу:
\begin{equation}
p_A^G=\sum\limits^{2}_{j=0}{p_A^G(4,j)  + p_A^G(3,3)\sum\limits^{\inf}_{n=0}p_a^{DG}(n + 2, n)}
\end{equation}
Пусть $p_A^{DG}(n + 2, n)$ - вероятность того, что игрок А выиграет с преимуществом в 2 мяча после того как был достигнут счет [3,3] на подаче игрока А.
\begin{equation}
p_A^{DG}(n+2,n)=\sum\limits^{n}_{j=0}{(p_A^Rq_A^R)^j(q^R_Ap^R_A)^{n-j}\frac{n!}{j!(n-j)!}}(p^R_A)^2=(p^R_A)^2(p^R_Aq^R_A)^n2^n
\end{equation}

В результате из формул 1 и 2 можно вывести следующую формулу -  вероятность выигрыша игроком А гейма на своей подаче.
\begin{equation}
p_A^{G} = (p^R_A)^4(1 + 4q^R_A + 10(q^R_A)^2) + 20(p^R_Aq^R_A)^3(p_A^R)^2(1 - 2_A^Rq^R_A)^{-1}
\end{equation} 
К
\subsubsection{Вероятность выиграть сет}
Пусть $p_A^S$ - вероятность того, что игрок А выиграет сет у игрока В, если игрок А начинает подавать первым, $q_A^S=1 - p^S_A$. Чтобы вывести $p^G_A$ из $p^G_A$ и $p^G_B$, определим $p_A^S(i,j)$ как вероятность того, что в розыгрыше сета игрок А выиграет i геймов, игрок B j геймов . Тогда
\begin{equation}
p_A^S=\sum\limits^{4}_{j=0}{p_A^S(6,j)} + p^S_A(7,5) + p^S_A(6,6)p^T_A\label{eq1}
\end{equation}
Где $p^T_A$  вероятность того, что игрок А выиграет 13-очковый тай-брейк , начинающийся с подачи игрока А.
\subsubsection{Вероятность выиграть матч}
Пусть $p^M_A$ - вероятность того, что игрок А выиграет матч у игрока B. Определим $p_{AB}^M(i,j)$ как вероятность, что игрок а выиграет в матче после того как количество выигранных им сетов достигнет i, а количество выигранных сетов у игрока B будет j, где матч начинается с подачи игрока А, а заканчивается на подаче B. Аналогично - $p^M_{AA}$.

Аналогично зададим вероятности победы для сетов $p^S_{AB}, p^S{AA}, p^S_{BA}, p^S_{BB}$, где $P^S_{XY}$ - вероятность того, что игрок сет закончится выигрышем игрока Х, начинающийся с подачи Х и заканчивающийся на подаче У.
Очевидно, что количество разыгранных геймов для $p^S_{XX}$ будет нечетным. Ограничим формулу \eqref{eq1}
\begin{equation}
p_A^S=\sum\limits_{j=1,3}{p_A^S(6,j)}  + p^S_A(6,6)p^T_A
\end{equation}

Если игрок Х подает в первом сете матча, а Y в последнем, количества геймов в сете будет нечетным.
Преобразуем \eqref{eq1}.
\begin{equation}
p_A^S=\sum\limits_{j=0,2,4}{p_A^S(6,j)} + p^S_A(7,5) 
\end{equation}

Итоговая формула для матчей где для достижения победы надо первым выиграть 3 сета(матчи мужского тенниса), будет выглядеть следующим образом(с более детальным выводом формул можно ознакомиться в \cite{Book23}, \cite{Book09}).
\begin{equation}
p_A^M=\sum\limits^{2}_{j=0,2,4}{(p^M_{AA}(3,j) + p^M_{AB}(3,j))} 
\end{equation}
Для матчей до 2 выигранных сетов(женский теннис)
\begin{equation}
p_A^M=\sum\limits^{1}_{j=0}{(p^M_{AA}(2,j) + p^M_{AB}(2,j))} 
\end{equation}
\subsection{Методы попарного сравнения}
Основная идея методов попарного сравнения заключается в том, что каждого игроку присваивается некий положительный рейтинг, который характеризует уровень его навыков. Эти методы основаны на алгоритме, предложенном Бредли-Терри\cite{Book17} и адаптированном для тенниса Ианом МакХолом\cite{Book18}.
Основная идея заключается в том, что вероятность победы игрока А над игроком B рассчитывается как
\begin{equation}
p_A=\frac{\alpha_A}{\alpha_A + \alpha_B}\label{eq2}
\end{equation}
Где $\alpha_A$ - рейтинг игрока А, а $\alpha_B$ - игрока В. Вариативность алгоритмов данного класса достигается за счёт различных методик подсчёта этого рейтинга. Например, при помощи ЭЛО-рейтинга\cite{Book09}. Рассмотрим один из вариантов - подсчёт рейтинга, основанный на количестве выигранных геймов. Преимуществом этого метода является, тот факт, что берется в расчёт информация о ходе матча. Т.е. разница между матчам проигранным со счётом 6-0,6-0 и со счётом 7-6, 7-6 существенна.
Тогда вклад можно оценить по следующей формуле
\begin{equation}
p^M_A=L(\alpha_A,\alpha_B) ∝\frac{\alpha_A^{g_a}\alpha_b^{g_b}}{(\alpha_A + \alpha_B)^{g_A + g_B}}
\end{equation} 
где $g_A$-количество выигранных геймов игроком А, а $g_B$ -  игроком В(тай-брейк  считается за обычный гейм).
Приведем формулу для расчёта рейтинга игрока из одной из последних работ, посвященной данному классу методов
\begin{equation}
% Z = A
L(\alpha_a(t,); a=1,...n) = \prod_{k\in Z_t}(\frac{\alpha_a(t,S)^{g_A} + \alpha_B(t,S)^{g_B}}{(\alpha_A(t,S) + \alpha_B(t,S))^{g_A + g_B}})^{exp(\epsilon( t-t_k))S_k}
\end{equation}
Где t - время матча на поверхности теннисного корта S, k - индекс сыгранного матча, $t_k$ - продолжительность матча k, $Z_t={k : < t_k < t}$, n - количество игроков в моделе, $\epsilon$ - параметр зависящий от S.

\subsection{Машинное обучение}
Машинное обучение - это область искусственного интеллекта (ИИ), которая изучает алгоритмы, которые обучаются на основе данных.
Алгоритмы машинного обучения с учителем ставят свой задачей вывести получить функцию пребразования данных из помеченных данных, где помеченные данные представляют собой пары : вектор значений  - результат.
В теннисе обычно используют исторические данные для формирования данных для обучения. Например, вектор значений может содержать информацию о матче и игроках, а результатом соотвественно будет служить исход матча. Выбор оптимальных параметров для формирования вектора напрямую влияет на точность получаемой функции.
К проблеме прогнозирования теннисных матчей можно подойти двумя способами:
\begin{itemize}
	\item Регрессионный подход, где результатом будет являться дейтвительное число - вероятность выиграть матч. Вероятности выиграть матч неизвестны для исторических данных, что вынуждает помечать их в исторических данных метками 0 и 1.
	\item Подход бинарной классификации. Тогда результатом работы алгоритма будет предсказание выигрыша или проигрыша матча.
\end{itemize}
\break
\subsubsection{Логистическая регрессия}
Несмотря на своё названия, логистическая регрессия является алгоритмом классификации. Свойства логической функции являются ключевыми для алгоритма. Логистическая функция определяется как:
\begin{equation}\label{formula3}
\sigma=\frac{1}{1 + e^{-t}}
\end{equation}
Логистическая функция отображает 
вещественные числа в диапазоне $(-\inf, +\inf)$ в диапазон $[0,1]$. Выходное значение логистические функции может интерпретироваться как вероятность.

Логистическая регрессия для прогнозирования матчей состоит из N свойств $x=(x_1,x_2, ..., x_n)$ и вектора
n+1 вещественных параметров модели $\beta=(\beta_0,\beta_1,...,\beta_n)$. Чтобы сделать прогноз при помощи модели, нужно преобразовать точке в n-мерном пространстве свойств  в вещественной число
\begin{equation}
z=\beta_0 +\beta_1x_1+\beta_2x_2+...\beta_nx_n
\end{equation}
Подставляя z в формулу \ref{formula1} получим 
\begin{equation}\label{formula1}
p=\sigma(z)=\frac{1}{1 + e^{-z}}
\end{equation}
Обучение модели состоит в оптимизации параметров $\beta$, таким образом, чтобы модель "как можно точнее" воспроизводила результаты матчей из данных для обучения. Это делается при помощи минизации функции логистических потерь \ref{formula2}, которая показывает меру ошибку прогнозирования результатов матчей тренировочных данных.
\begin{equation}\label{formula2}
L(p)=\frac{-1}{N}\sum_{i=1}^Np_ilog(y_i) + (1 - p_i) + (1 - p_i)log(1-y_i)
\end{equation}
Где N - количество матчей в обучающих данных, $p_i$ - предсказанная вероятность выиграть матч i,  $y_i$ - фактический исход матча(0 - поражение, 1 - победа).

 В зависимости от размера датасета, выбирается одна из двух функций минимизации:
 \begin{itemize}
 	\item Стохастический градиентный спуск - медленный итеративный метод, подходящий для больших датасетов
 	\item Метод максимального правдоподобия - быстрый численный метод, не подходящий для работы с большими датасетами.
 \end{itemize}
Большинство публикаций, в которых используется методы машинного обучения для прогнозирования теннисных матчей, используют логистическую регрессию. Например, Кларк и Дит\cite{Book10} построили логистическую регриссионную модель, основанную на разнице набранных очков в рейтинге ATP для прогнозирования результатов розыгрыша сета. Другими словами, они использовали одномерную регрессию x=(разница\_в\_рейтинге) и оптимизировали $\beta$ таким образом, чтобы функция $\sigma(\beta_1x)$ показала наилучший результат на обучающем датасете.Параметр $\beta_0$ был исключён их модели на основании того, что разница в рейтинге 0 может давать вероятность выигрыша матча 0.5. Вместо того, чтобы прогнозировать результаты матча, они предпочли предсказывать результаы каждого конретного сета, тем самым увеличивая набор данных для обучения.

Ма, Лу и Тан\cite{Book11} использовали многомерную регрессию, использующую 16 свойств, поделённых на 3 категории: умения игрока, характеристики игрока и характеристики матча. Модель была обучена на матчах между 1991 и 2008 годом и была использована для улучшения тренировок игроков.

Логистическая регрессия привлекает исследователей тем, что она позволяет быстро обучать модели, почти не влечёт проблем переобучения моделей, а так же позволяет получить легко интерпретируемый выходной параметр - вероятность выиграть матч. Тем не менее, при наличии сложных взаимосвязей между входными параметрами, использование логистической регрессии может стать нетривиальной задачей.

\subsubsection{Нейронные сети}
Искуственная нейронная сеть - это система взаимосвязанных "нейронов"\, вдохновлённая биологическими нейроннами. Каждый нейрон преобразует входные данные в какое-то выходное значение, которое в дальнейшем могут быть использованы в качестве входных данных для других нейронов. Нейронная сеть обычно имеет несколько слоёв,c нейроном на каждом не-входном слое, соединённом с нейронами в предыдущих слоях.
\begin{figure}[!h]
	\centering
	\includegraphics[width=.9\textwidth]{img/img1.png}
	\caption{Трехслойная нейронная сеть с прямыми связями(ПЕРЕВЕСТИ КАРТИНКУ)}
	\label{fig01}
\end{figure}
Каждая связь в сети имеет свой вес. Нейрон использует входные значения и веса чтобы посчитать выходное значение. Типичным методом композиции является  взвешенная сумма.

\begin{equation}
f(x)=K(\sum_iw_ix_i)
\end{equation}
Нелийнейная функции активации К позволяет нейронной сети решать нетривиальный задачи, используя небольшое количество нейронов. Логистическая функция \ref{formula3} может использоваться в качестве функции активации.

Прогноз результата матча осуществляется на основе передачи в нейронную сеть неких свойств игрока и свойств матча. Если используется логичестическая функция активации, то выходное значение нейронной сети можно интерпретировать как вероятность выиграть матч. Есть большое количество различных алгоритмов обучения, которые оптимизируют набор весов нейронной сети в целях получении наилучшего выходного значения. Например, алогритм обратного распространения использует градиентный спуск для уменьшения среднего квадрата ошибки между целевыми значениями и выходами сети.

Например, Сомбоонфокафан\cite{Book12} сконструировал трёхлойную сеть прямой связи, использую алгоритм обратного распространения ошибки. Протестировал на различных архитектурах сети и различных наборах свойств. Нейронная сеть, показавшая наилучший результат, имеет 27 входных нейронов. На вход подавались свойства, описывающие обоих игроков, а так же свойства матча. В результате была получена точность предсказания 75\% на исторических данных турниров "Большого шлема"\ в 2007-2008 годах.

Нейронная сеть можно установить наличие сложных взаимотношений между различными свойствами, подаваемыми на вход. Но она работает по принципу "черного ящика"\, т.е. механизм определения таких взаимоотношений не будет известен. К недостаткам нейронных сетей так же можно отнести то, что они склонны к переобучению, зачастую требует большого количества данных для обучения. Кроме того, разработка структуры сети зачастую является эмпирической и выбор гиперпараметров модели часто идёт методом проб и ошибок.
\subsubsection{Метод опорных векторов}
Основная идея метода опорных векторов заключается перевод исходных наборов свойств  в пространство более высокой размерности и поиск разделяющей гиперплоскости с максимальным зазором в этом пространстве. Две параллельных гиперплоскости строятся по обеим сторонам гиперплоскости, разделяющей классы. Разделяющей гиперплоскостью будет гиперплоскость, максимизирующая расстояние до двух параллельных гиперплоскостей. Алгоритм работает в предположении, что чем больше разница или расстояние между этими параллельными гиперплоскостями, тем меньше будет средняя ошибка классификатора.
На данный момент в открытом доступе нет работ по прогнозировани теннисных матчей при помощи метода опорных векторов\cite{Book13}. Метод опорных векторов имеет несколько преимуществ по сравнению с нейронными сетями. Во-первых, обучение не заканчивается на локальном минимуме, что зачастую бывает с нейронными сетями. Во-вторых, при наличии большого количество данных для обучения, метод опорных векторов обычно превосходит нейронные сети в прогнозировании \cite{Book13}. Тем не менее, время обучения модели метода опорных векторов намного выше, а создание и задание необходимых параметров модели является нетривиальной задачей.

\subsubsection{Проблемы машинного обучения}
Наличие большого количества данных для обучения не означает получение более точных результатов прогнозирования результатов теннисных матчей, т.к. большое количество данных означает, что теннисист играл много лет матчи. Со временем его навыки меняются и результаты последних матчей представляют набольший интерес. Так же в теннисе важную роль играет покрытие на которой играется матч, т.е. для модели необходимы данные о последних матча игрока именно на этом покрытии. Поэтому машинное обучение может страдать от недостатка данных, что может приводить к переобучению модели, с которым может быть сложно бороться. В частности нейронные сети сильно подвержены переобучению если количество слоёв/нейронов сравнительно велико относительно размера данных для обучения.

Чтобы бороться с проблемой переобучения, нужно использовать только важные свойства для обучения. Процесс отбора важный свойств набора данных называется "отбор признаков"\, для этого существует большое количество различных алгоритмов. Удаление незначащих свойств значительно уменьшает время обучения
\subsubsection{Оптимизация гиперпараметров}
Модель имеет параметры как полученаемы в процессе обучения путём их оптимизации(например, веса в нейронных сетях), так и статически задаваемые параметры такие как количество скрытых слоёв, количество нейронов в каждом слоё. Статически настраиваемые параметры назвают гиперпараметрами. Процесс получения оптимальных гиперпараметров может быть как эмпирическим, так и алгоритмическим. Пример алгоритмического поиска гиперпараметров - поиск по сетке. Он происходит на заранее определённом пространстве гиперпараметров. Для построения модели прогнозирования с высокой точностью, необходимо тщательно подобрать необходимые гиперпараметры.

\subsubsection{Выбор оптимальных свойств}
Каждый набор данных может содержать большое количесто различных свойств. Каждое свойство вносит определенный вклад в итоговый результат. Выбор оптимального количества наиболее значимых свойств - приём очень часто применяемый в машинном обучение. От количества свойств зависит производительность получаемой модели. Свойства, не влияющие на итоговый результат, или влияющие незначительно, негативно влияют на производительность модели, при влючении их в оную. Так же нерелевантные свойства могут ухудшуить точность модели, заставить её обучаться на нерелевантных данных\cite{Book14}.

Преимущества выбора оптимального набора свойства
\begin{itemize}
	\item Уменьшаеть вероятность переобучения. Испольнование меньшего количества избыточных данных уменьшает вероятность принятия решения на основе "шума"\ .
	\item Повышение точности.
	\item Сокращает время обучения- меньшее количество данных снижают сложность модели.
\end{itemize}
Выбор свойств может производится как в ручном, так и автоматическом режиме.

Существуют 3 основных класса алгоритмов выбора свойств.
\begin{itemize}
\item Фильтр. Рассчитывается определенная метрика и свойства "фильтруются" на основании этой метрики
\item Обертка. "Оберточные"\ методы рассматривают выбор оптимального набора свойств
\end{itemize}

\textbf{Корреляция Пиросона} - фильтр-метод, который использует в качестве метрики следующую формулу:
\begin{equation}
r=\frac{\sum(x - \overline{x})(y - \overline{y})}{\sqrt{\sum(x - \overline{x})^2\sum(y - \overline{y})^2}}
\end{equation}

\textbf{Хи-квадрат метод} - фильтр-метод, который использует в качестве метрики следующую формулу:
\begin{equation}
\chi^2=\sum_{i=1}^n\frac{(O_i - E_i)^2}{E_i}
\end{equation}
Где $O_i$ - фактическое количество наблюдений класса i, а $E_i$ - ожидаемое количество наблюдей класса i, при условии отсутствия зависимости между свойством и ожидаемым значением.
Рассмотрим пример подсчёта метрики Хи-квадрат.
Предположим, у нас есть следующие данные
\begin{table}[!h]
	
	\caption{\label{tab:table1}Фактическое количество наблюдений результатов игроков}
	
	\begin{center}
\begin{tabular}{|l|l|l|l|}

\hline

  & Кол-во побед & Кол-во поражений & Всего \\
\hline
Игрок1  & 20 & 5 & 25 \\
\hline
Игрок2  & 40 & 35 & 75 \\
\hline
Всего  & 60 & 40 & 100 \\
\hline

\end{tabular}
		
\end{center}

\end{table}

Для того чтобы посчитать значения $\chi^2$, посчитаем "ожидаемое"\ значение в каждой ячейки, если бы свойства были дейтвительно независимы. Для этого нужно просуммировать значения каждой строки и поделить на общее количество записей.
Получим следующую таблицу;
\begin{table}[!h]
	
	\caption{\label{tab:table2}Ожидаемое количество наблюдений результатов игроков}
	
	\begin{center}
\begin{tabular}{|l|l|l|l|}

\hline

  & Кол-во побед & Кол-во поражений & Всего \\
\hline
Игрок1  & 15 & 10 & 25 \\
\hline
Игрок2  & 45 & 25 & 75 \\
\hline
Всего  & 60 & 40 & 100 \\
\hline

\end{tabular}
		
\end{center}

\end{table}

\textbf{Лассо-метод(сокращение от англ. least absolute shrinkage and selection operator)} - встроенный метод. На основе линейной регрессии происходит пошаговый выбор оптимальных параметров модели.  Идея метода заключается в том, что накладывается ограничение на сумму абсолютных значений свойств модели, сумма должна быть меньше некоторого фиксированного значения. Для этого того, чтобы этого добиться, применяется методы сжатия(регуляризации), в процессе которого коэффициенты регрессионных переменных уменьшаются. Свойства, получившие в результате регуляризации нулевой коэффициент, отбрасываются.

\textbf{Методы на основе деревьев} относятся к классу встроенных методов. Например, для определения оптимального наборы свойств может использоваться алгоритму случайного леса. Принцип работы встроенных методов нетривиален и выходит за рамки данной работы, подробнее с их принципами работы можно ознакомится в \cite{Book15}
\subsection{Предварительная обработка данных}

Перед тем как начать с работать с данными, их нужно предварительно обработать.Данные могут хранится в числовом, текстовом виде, в формате изображений, аудио/видео файлов и других многочисленных форматах. Алгоритмы прогнозирования работают исключенительно с числовыми данными. Поэтому данные в других форматах нужно перевести в числовой формат. В данной работе будет рассмотрено как перевести текст В данной работе мы будем рассматривать только данные, которые хранятся в текстовом виде. 
Перед тем как данные переводятся в числовой вид, они предваритально очищаются. 
Для этого нужно
\begin{itemize}
	\item Удалить все нерелевантные символы (например, любые символы, не относящиеся к цифро-буквенным).
	\item Токенизировать текст, разделив его на индивидуальные слова.
	\item Удалить нерелевантные слова — например,  гиперссылки.
	\item Перевести все символы в нижний регистр для того, чтобы слова «привет», «Привет» и «ПРИВЕТ» считались одним и тем же словом.
	\item Рассмотрите возможность совмещения слов, написанных с ошибками, или имеющих альтернативное написание (например, «круто»/«круть»/ «круууто»)
	\item Рассмотрите возможность проведения лемматизации, т. е. сведения различных форм одного слова к словарной форме (например, «машина» вместо «машиной», «на машине», «машинах» и пр.)
	\item Рассмотреть возможность удаления слов, не несущих самостоятельной смысловой нагрузки. Например, артиклей в английском языке.
	\item Рассмотреть удаления слов не существенных для решаемой задачи на основание каких-либо параметров.
\end{itemize}
\begin{figure}[!h]
	\centering
	\includegraphics[width=.5\textwidth]{master_img/learning-word-representations.jpg}
	\caption{Схема перевода текста в слова}
	\label{fig01_master}
\end{figure}

\subsubsection{One-Hot Encoding}
\begin{figure}[!h]
	\centering
	\includegraphics[width=.5\textwidth]{master_img/word2vec-one-hot.png}
	\caption{Представляем предложение "The queen entered the room" в виде вектора}
	\label{fig02_master}
\end{figure}
One hot encoding - это процесс в помощью которого некоторые категоральные переменные(в нашем случае - слова), преобразуются в форму, понятную алгоритмам машинного обучения. Берется достаточно большой набор заранее известных слов(обычно речь идёт о десятках тысячах слов). На основании этого набора создаётся вектор чисел, где каждому слову соотвествует определенная позиция в этом векторе с изначальным  значением 0. Текст проверяется на наличие таких слов. Если слово присутсвует в тексте, то соотвествующая позиция устанавливает в 1, иначе в 0. На выходе получается разряженная матрица в виде вектора. Так же некоторые модификации этого метода используют размерности матрицы отличные от 1 по высоте.
К недостаткам данного метода можно отнести:
\begin{itemize}
	\item Большое количество затрачиваемой памяти. Текст из 40 слов и 4000 будут заниматься одинаковое количество место. Решается оптимизацией хранения разреженных матриц
	\item Потерю смысла слов. Одно и тоже слово может иметь несколько значений в зависимости от контекста
	\item Потерю порядка слов.
	\item Потеря количества определеннных слов в тексте. Слово, употребленное в тексте один раз, имеет такой же вес, как и многократно употреблённое слово.
\end{itemize}
Так же к недостаткам стоит отнести трудоёмкость предварительной подготовки текста.
Для этого метода необходимо привести слова в один падеж, одно число.Например, "кота"\ - > "кот"\, "коты"\ -> "кошка"\ . Так же слова-синонимы заменяются на один конкретный вариант. Например, "автомобиль"->"машина". Это может быть нетривиально, особенно когда синонимы состоят из нескольких слов.

Основным преимуществом данного метода считается, что полученных данных достаточно для решения многих задачл ествественной обработки языка\cite{Book26}, в частности классификации. Т.е. для многих задач анализа естественного языка вышеописанные недостатки несущественны. Так же такой формат данных удобен для работы с нейронными сетями и другими алгоритмами машинного обучения.

Развитием алгоритма one-hot encoding является так называемый "мешок слов"\ . Для его получения OHE-вектора каждого слова в тексте складываются. .е. на выходе получим просто подсчет количества различных слов в тексте в одном векторе. Такой подход называется "мешок слов"\ (bag of words, BoW), потому что мы теряем всю информацию о взаимном расположении слов внутри текста.
\begin{figure}[!h]
	\centering
	\includegraphics[width=.5\textwidth]{master_img/wordbag.jpg}
	\caption{Мешок слов}
	\label{fig03_master}
\end{figure}
\subsubsection{word2vec}
Вышеописанные подходы были (и остаются) хороши для областей, где количество текстов мало и словарь ограничен, хотя, конечно, они имеют свои недостатки Но в современном мире количество слов огромно, слова постоянно появляются и исчезают С этим надо было что-то делать, а ранее известные модели не могли справиться с таким объемом текстов. Предположим, что в английском языке сейчас примерно около миллиона слов. Матрица совместных встречаемостей(такие матрицы используются для сравнения текстов) только пар слов будет  иметь размер $10^6 x 10^6$. Такие матрицы занимают очень много памяти, с ними неудобно работать. Конечно,  придумано множество способов, упрощающих или распараллеливающих обработку таких матриц, всё-таки до сих пор с ними крайне тяжело работать\cite{Book27}.


Поэтому в 2013 году был предложен новый подход к word embedding, названный word2vec.
Этот подход основан на гипотезе, которую в науке принято называть гипотезой локальности — Эслова, которые встречаются в одинаковых окружениях, имеют близкие значенияЭ. Близость в данном случае понимается очень широко, как то, что рядом могут стоять только сочетающиеся слова. Например, для нас привычно словосочетание "заводной будильник". А сказать “заводной арбуз”  нельзя — эти слова не сочетаются.

Модель, предложенная Миколовым\cite{Book28}, проста. Она основывается на  предсказании вероятности слова по его окружению (контексту). То есть ожидается получение таких векторов слов, чтобы вероятность, присваиваемая моделью слову была близка к вероятности встретить это слово в этом окружении в реальном тексте.
Приведем формулу "мягкого максимума"\ (дифференцируемого). Максимум должен быть дифференцируем, чтобы модель могла обучаться при помощи алгоритма обратного распространения ошибки.
\begin{equation}
P(w_o|w_c) = \frac{e^{s(w_0, w_c)}}{\sum_{w_i \in V}e^{s(w_0, w_c)}}
\end{equation}
Где $w_0$ - вектор целевого слова, $w_c$  - некоторый вектор контекста вычисленный(например, путем усреднения) из векторов окружающих данное слово слов, $s(w_1, w_2)$  - некая функция, получающая на вход 2 вектора и отображающая их в одно число. Например, это может быть функция косинусного расстояния.

Модель обучается следующим образом: берется последовательность 2k + 1 слов, где слово в центря является словом, которое должно быть предсказано. Окружащюие слова являеются контекстом длины K с каждой стороны от искомого слова. В процессе обучения каждому слову сопоставляется уникальный вектор, который меняется в процессе обучения.

Такой подход называется CBOW(англ. continuous bag of words). Является подвидом метода "мешка слов"\ т.к. порядок слов в контексте не учитывается. 

Так же существует развитие данного метода - skip-gram\cite{Book28}("словосочетание с пропуском"\ ). Оно основывается на том, что  из данного нам слова мы пытаемся "угадать контекст"\ . В остальном алгоритм не претерпевает измненений.

Натренированная модель word2vec может улавливать некоторые семантические и синтаксические свойства слов, несмотря на то что в модель явно не заложено никакой семантики.
\begin{figure}[!h]
	\centering
	\includegraphics[width=.5\textwidth]{master_img/word2vec_example.png}
	\caption{word2vec пример семантической корреляции\cite{Book28}}
	\label{fig04_master}
\end{figure}
Слово "мужчина"(англ. man) относится к слову "женщина"(англ. woman) так же, как слово "дядя"(англ. uncle) к слову "тётя"(англ. aunt). Для человека это естественно и понятно, но в других моделям добиться такого же соотношения векторов можно только с помощью специальных ухищрений.
\subsubsection{Улучшения word2vec}
Модель CBOW c функцией оптимизации(минимизацией) в виде  дивергенции Кульбака-Лейблера называют моделью CBOW с negative sampling\cite{Book29}.
\begin{equation}
KL(p||q) = \int p(x)log\frac{p(x)}{q(x)}dx
\end{equation}
Где $p(x)$  — распределение вероятностей слов, а $q(x)$ - распределение словвзятое из модели(которая была получена на основе текста). Эта формула имеет описывает насколько одно распределение не похоже на другое(англ. divergence  - расхождение). Т.к. распределение получается на основе подсчёта слов, оно является дискретным. Поэтому  можно заменить интеграл на сумму:
\begin{equation}
KL(p||q) = \sum_{x \in V} p(x)log\frac{p(x)}{q(x)}dx
\end{equation}
К сожалению, эта формула имеет недостаток - большую вычислительную сложность. Прежде всего, из-за того, что $q(x)$ рассчитывается как softmax на основании всего словаря текста.
Именно поэтому Томасом Милковым\cite{Book29} была предложена оптимизация вышеприведенной формула, названная Negative Sampling. Как уже отмечалось ранее, очевидно, что многие слова вместе не встречаются. Поэтому большая часть вычислений в softmax являются избыточными. Это позволяет ввести следующую оптимизацию: максимизируется вероятность встречи слова для нужного слова в типичном для него контексте(в том котором он чаще всего встречается в заданном тексте). А вероятность встречи слова в нетипичном для него контексте, соотвественно, минимизируется.
\begin{equation}
NegS(w_0) = \sum{_{i=1, x_i \sim D}^{i=k} -log(1 + e^{s(x_i, w_0)})}  + 
\sum{_{j=1, x_j \sim D}^{i=k} -log(1 + e^{s(x_j, w_0)})}
\end{equation}
Где $s(x,w)$ - аналогичен оригинальной формуле. Формлуа состоит из двух частей - $+s(x,w)$ и негативной $-s(x,w)$
\subsection{Нормализация данных} 
После получения предварительно обработанного датасета для обучения модели, данные необходимо нормализовать. Нормализация данных предназначена для устранения зависимости от выбора единицы измерения и заключается в преобразовании диапазонов значений всех атрибутов к стандартным интервалам([0,1] или [-1,1])\cite{Book25}. Нормализация данных направлена на придание всем атрибутам одинакового "веса".
\subsubsection{Основные методы нормализации данных}
\begin{enumerate}
	\item Min-max нормализация заключается в применении к диапазону значений атрибута х линейного преобразования, которое отображает [min(х),max(х)] в [A,B].
	\begin{equation}
	x^\prime_i=\uptau(x_i)=\frac{x_i - min(x)}{max(x) - min(x)}*(B-A) + A
	\end{equation}
	\begin{equation}
	x \in[min(x), max(x)] \Rightarrow \uptau(х) \Rightarrow [A,B]
	\end{equation}
	Min-max нормализация сохраняет все зависимости и порядок оригинальных значений атрибута. Недостатком этгого метода является то, что выбросы могут сжать основную массу значений к очень маленькому интервалу
	\item Z-нормализация  основывается на приведении распределения исходного атрибута х  к центрированному распределению со стандартным отклоненим, равным 1 \cite{Book25} .
	\begin{equation}
	x^\prime_i=\uptau(x_i) =\frac{x_i - \overline{x}}{\sigma_x}
	\end{equation}
	\begin{equation}
	M[x^\prime]=1	 
	\end{equation}
	\begin{equation}
	D[\overline{x}^\prime]=0	 
	\end{equation}
	Метод полезен когда в данных содержатся выбросы.
	\item Масштабирование заключается в изменении длины вектора значений атрибута путем умножения на константу \cite{Book06} .
	\begin{equation}
	x^\prime_i=\uptau(x_i)=\lambda*x_i
	\end{equation}
	Длина вектора х уменьшается при $|\lambda|<1$ и увеличивается при $|\lambda|>1$ 
\end{enumerate}