\chapter{Аналитический раздел}
\label{cha:analysis}
\section{Цель и задачи работы}
Целью данной работы является создание программного комплекста для обнаружения выбросов временных рядов в наборах данных.
Для достижения данной цели необходимо решить следующие задачи:
\begin{itemize}
	\item пронализировать предметную область и существующие методы обнаружения выбросов
	\item разработать метод обнаружения выбросов
	\item создать ПО, собирающее данные для анализа
	\item создать ПО, реализующее  разработанный метод обнаружения выбросов
	\item провести исследование с использованием разработанного метода
	
\end{itemize}
\section{Что такое аномалия}
В анализе данных есть два основных направления, которые занимаются поиском аномалий - это детектирование новизны и обнаружение выбросов. "Объект новизны"\ - это так же объект, который отличается по своим свойствам от объектов  выборки. Однако, в отличие от выброса,  его ещё нет в самой выборке и задача анализа сводится к его обнаружению при появление. Например, если анализировать замеры уровня шума и отбрасывать слишком высокие или слишком низкие значения, то это называется борьбой с выбросами. А если  создаётcя алгоритм, который для каждого нового замера оценивает, насколько он похож на прошлые, и выбрасывает аномальные, то это  назвается "борьбой с новизной"\ 
\cite{Book01}.
Выбросы являются следствием:
\begin{enumerate}
	\item ошибок в данных
	\item неверно классифицированных объектов
	\item присутсвием объектов других выборок
	\item намеренным искажением данных
\end{enumerate}
На рисунке \ref{fig01} находится три вида точек: зеленые, желтые, красные. Множество зеленых точек представляют собой "нормальные"\ данные. Множество желтых точек означает  выбросы в "слабом смысле". Они незначительно отклоняются от основных  нормальных данных. Красные же точки являются аномальными - выбросами "в сильном смысле"\ . Они значительно  отклоняются  от нормальных данных. В данной работе будет изучаться вопрос нахождения "сильных выбросов"\  и  критериев отличия сильного выброса от основных данных. В дальнейшем под словом "выброс"\ будет подразумеваться "сильный выброс"\ ,  а под  аномалией -  выброс(выброс - частный случай аномалии).
Понятие аномалии  интерпетируют по-разному в зависимости от характера данных. Обычно аномалией назыют некоторое отклонение от нормы. В дальнейшем будет дано несколько более формальных опредений аномалий, специфичных для метода их определений.

\section{Обнаружение аномалий}
В машинном обучении обнаружение  "ненормальных"\ экземпляров в наборах данных всегда представляло большой интерес. Вероятно, первое определение было дано Граббсом\cite{Book02} в 1969 году: "Относительное наблюдение или выброс - это элемент выборки, который, заметно отличается от других членов выборки, в которых он встречается "\ .
Это определение является актуальным и сегодня, но мотивация для обнаружения аномалий изменилась. Тогда основная причина поиска аномалий заключалась в том, чтобы удалить выбросы из набора данных для обучения, поскольку   используемые алгоритмы, были весьма чувствительны к выбросам в данных. Эта процедура также называется очищением данных. После разработки классификаторов устойчивых к наличию аномалий в обучающем наборе данных, интерес к их поиску угас. Однако, в начале 21 века в связи с развитием интернета и значительным увеличением объема собираемых данных для анализа, исследователи стали больше интересоваться  аномалиями, поскольку они  оказывались часто связаны с особенно интересными событиями.  В этом контексте определение Граббса также было расширено, так что сегодня аномалии имеют две важные характеристики:
\begin{enumerate}
	\item Аномалия отличается от нормы по своим особенностям
	\item Аномалия редко встречается в наборе данных по сравнению с "нормальными"\  данными
\end{enumerate}
\subsection{Классификация методов обнаружений аномалий}
Классическая система классификации предполагает предварительное обучение на обучающем наборе данных и последующую классификацию на основе этого набора. Данные делятся на "обучающую выборку"\ - данные, при помощи которых алогритм обучает классификар и, "тестовую выборку"\ - данные, при анилизе которых классификатор остается неизменным. Тестовая выборка нужна для того чтобы проверить корректность обучения классификатора.

 Однако, в случае с поиском аномалий, возможны варианты, отличающиеся от классического. Подходящий метод классификации выбирается на основе наличия разметки данных.   Выделяются три основых класса методов:
\begin{enumerate}
\item Обучение с учителем. Для обучения необходимо наличие полностью  размеченных данные для обучения и для тестов. Классификатор  обучается один раз и применяться впоследствии.В связи с тем, что для многих наборов данных заранее неизвестно что является аномалией, а что нет, применение этого метода ограничено.
\item Обучение с частичным привлечением учителя. Для обучения необходимо наличие тествого и учебного набора данных. Однако, в отличие от обучения с привелечением учителя, разметка данных не требуется. Все данные, представленные в выборках, считаются нормальными. На основе этих данных строится некая модель. Все данные, отклоняющиеся от этой модели, считаются аномальными. Эта идея также известна как "одноклассовая"\ классификация \cite{Book03}.
\item Обучение без учителя.
Самый гибкий способ, который не требует разметки набора данных.  Идея заключается в том, что алгоритм обнаружения аномалий оценивает данные исключительно на основе внутренних свойств набора данных что является нормальным, а что является выбросом. В данной работе основное внимание будет этому  именно этому способу. Так же этот способ называют "неконтролируемый способ обнаружения  аномалий".
\end{enumerate}

\section{Результат метода обнаружения аномалий}
В результате работы алгоритма обнаружения аномалий  с элементом данных связывается  метка или оценка достоверности(показатель аномальности).  Метка - показатель, который принимает нулевое значения, в случае если она связана с нормальными данными и единицу в противном случае. Оценка показывает вероятность того, что элемент является аномалией. Для разных алгоритмов используется разные шкалы оценок, поэтому приведение конкретных примеров оценок будет некорректным.  В алгоритмах метода обучения с учителем зачастую используются метки как выходные данные, в  алгоритмах  с частичным привлечением учителя и без учителя  обнаружения аномалий чаще встречаются оценки.
\section{Виды аномалий}
\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{img/1_1.png}
	\caption{Аномалии в двумерном пространстве}
	\label{fig01}
\end{figure}

Основная идея алгоритмов обнаружения аномалий заключается в обнаружении экземпляров данных в наборе данных, которые отклоняются от нормы. Однако на практике существует множество случаев, когда это основное предположение является неоднозначным. На рис \ref{fig01} показаны некоторые из этих случаев с использованием простого двумерного набора данных. Две аномалии могут быть легко идентифицированы визуально: красные  точки(обозначены треугольниками)  сильно отличаются отличаются значениям параметров от областей плотной группировки точек. Если смотреть на весь набор данных в целом, то фиолетовую точку(обозначена квадратом) можно отнести к тому же классу, что и зеленые точки(обозначены кругами).  Однако, если сфокусироваься только на кластере зеленых точек и сравнивать его с фиолетовой точкой, пренебрегая всеми другими точками, то её можно рассматривать как аномалию. Поэтому фиолетовая точка называется локальной аномалией, так как она аномальна по сравнению с ее близкой окрестностью. В зависимости от цели анализа, локальные  аномалии могут представлять интерес или нет. 
Другой  вопрос  заключается в том,что следует ли рассматривать точки черного кластера(обозначены звездочками)  как три аномалии или как (небольшой) кластер. Такие небольшие кластеры  называются микрокластерами. Показатели аномальности у точек этого кластера выше, чем  у точек зеленого кластера, но меньше, чем  у красных точек. Этот простой пример  показывает, что задача нахождения аномалий не всегда тривиальна, а вычисление показателя аномалности иногда полезнее, чем проставление двоичной метки.


Задача обнаружения одиночных аномальных экземпляров в крупном наборе данных называется обнаружением точечных аномалии\cite{Book04}.Большинство   неконтролируемых алгоритмов обнаружения  относятся к этому типу. Если же аномалии составляют заметный процент от набора данных, то задачу поиска аномалий называют задачей обнаружения коллективных аномалий. Пусть аномалии представляют собой некое множество, тогда необязательно каждый элемент этого множества должен быть аномальным. Возможен вариант когда только определенная их комбинация определяет аномалию.  Третий вид  -  контекстуальные аномалии. Элемент выборки в отрыве от своего контекста может казаться нормальным. Однако, если рассмотреть контекст этого элемента, то очевидным станет его аномальная природа.
 Распространенным контекстом является время. В качестве примера предположим, что  измеряется температура в диапазоне от $-30^{\circ}$C до $+40^{\circ}$C в течение года. Таким образом, температура $25^{\circ}$C кажется довольно нормальной, но когда  учитывается контекстное время (например, месяц), такая высокая температура $25^{\circ}$C  в течение зимы  будет рассматриваться как аномалия.

Алгоритмы обнаружения точечных аномалий также можно использовать для обнаружения контекстуальных и коллективных аномалий. Для этого нужно включить контекст в алгоритм как параметр алгоритма. В вышеприведенном примере включение месяца как дополнительного параметра поможет обнаружить аномалию. Однако в более сложных сценариях может потребоваться один или несколько новых параметров, чтобы преобразовать задачу определения контекстной аномалии в задачу обнаружения точечной аномалии.  Для того, чтобы преобразовать задачу поиска коллективной аномалии в задачу поиска одиночной, нужно произвести изменения начального набора данных. Для этого можно использовать корреляцию, агрегацию и группировку. Преобразование  может быть нетривиальным.\cite{Book05} . 
Преобразование требует глубоких знаний о наборе исходных данных и часто приводит  к существенным искажениям при переводе данных в новый формат. Такое семантическое преобразование называется  генерированием представления данных(\textit{англ. data view generation}).
  
  Таким образом можно сделать вывод, что многие задачи обнаружения аномалий требуют предварительной обработки данных перед передачей их на вход алгоритму. В противном случае можно получить формально верные, но фактические бесполезные результаты.
\subsection{Нормализация данных} 
После получения предварительно обработанного датасета для поиска точечной аномалии, последним шагом перед передачей в алгоритм, является нормализация данных. Нормализация данных предназначена для устранения зависимости от выбора единицы измерения и заключается в преобразовании диапазонов значений всех атрибутов к стандартным интервалам([0,1] или [-1,1])\cite{Book06}. Нормализация данных направлена на придание всем атрибутам одинакового "веса".
\subsubsection{Основные методы нормализации данных}
\begin{enumerate}
	\item Min-max нормализация заключается в применении к диапазону значений атрибута х линейного преобразования, которое отображает [min(х),max(х)] в [A,B].
	\begin{equation}
	x^\prime_i=\uptau(x_i)=\frac{x_i - min(x)}{max(x) - min(x)}*(B-A) + A
		\end{equation}
   \begin{equation}
		x \in[min(x), max(x)] \Rightarrow \uptau(х) \Rightarrow [A,B]
	\end{equation}
	Min-max нормализация сохраняет все зависимости и порядок оригинальных значений атрибута. Недостатком этгого метода является то, что выбросы могут сжать основную массу значений к очень маленькому интервалу
	\item Z-нормализация  основывается на приведении распределения исходного атрибута х  к центрированному распределению со стандартным отклоненим, равным 1 \cite{Book06} .
	\begin{equation}
	x^\prime_i=\uptau(x_i) =\frac{x_i - \overline{x}}{\sigma_x}
		\end{equation}
		\begin{equation}
		M[x^\prime]=1	 
		\end{equation}
		\begin{equation}
		D[\overline{x}^\prime]=0	 
		\end{equation}
		Метод полезен когда в данных содержатся выбросы.
	\item Масштабирование заключается в изменении длины вектора значений атрибута путем умножения на константу \cite{Book06} .
	\begin{equation}
	x^\prime_i=\uptau(x_i)=\lambda*x_i
	\end{equation}
	Длина вектора х уменьшается при $|\lambda|<1$ и увеличивается при $|\lambda|>1$ 
\end{enumerate}
\section{Неконтролируемые алгоритмы обнаружения аномалий}
Существуют различные классы методов обнаружения неконтролируемых аномалий. Выбор соответствующего класса метода зависит от характера данных и наличия априорной информации, требоваваний к скорости работы алгоритма, затрачиваемой памяти, размеру данных и многих других параметров. Так же можно заметить, что в рамках методы одного класса могут значительно отличаться по своим характеристикам, что дополнительно усложняет выбор. Рассмотрим основые классы методов, их преимущества и недостатки. 
\subsection{Вероятностно-генеративные методы}
Основная идея генеративных методов заключается в использование вероятностного смесевого моделирования данных. Предлагается подобрать такую вероятностую модель, из которой было получены нормированные данные. Такие модели обычно называются генеративными моделями, где для каждой точки(элемента данных) можем посчитать генеративную вероятность(или вероятность правдоподобия).Т.е. задача  сводится к нахождению плотности распределения p(x). Аномалиями при этом  считаются точки(элементы набора данных), имеющию низкое правдоподобие. В качестве показателя аномальности выступает функция p.
Для построения генеративной модели нужно решить следующую задачу:
	\begingroup
	\Large
	\begin{equation}
	\prod \limits_{x \in X_{norm}} p(x,\theta)  \rightarrow max_\theta
		\end{equation}
	\endgroup
		где \begingroup \Large$ X_{norm}$ \endgroup - нормальные данные представленного набора данных ${p(x,\theta)|\theta \in \omega}$ -семейство плотностей вероятностей, параметризованные $\theta$.
		
Этот метод редко используется на практике, так как тяжело проверить полученную генеративную модель на адекватность, сложно  убедится в правильном выборе семейства смесевых распределений. Это связано с тем, что низкое значение функции правдоподобия может означать как и аномальное значение, так и неудачно подобранную модель. Этот метод применяется с опорой на априорную информацию, в случае когда можно проверить полученную модель на адекватность.
\subsection{Линейные методы}
Основной идеей линейных методов является построение некой  модели, характеризующей нормальные данные. Точки, которые значительно отклоняются от этой модели, считаются аномалиями.

Предполагается, что нормальные данные  находятся в подпрострастрансве пространства атрибутов данных(размер подпространства атрибутов данных равен размерности данных). В свою очередь, задача линейного метода - найти низкоразмерные подпространства, такие что, выборка данных этого подпространства значительно отличается от остальных точек пространства данных.

Одним из возможных вариантов решения является использование линейной регрессии. Выбирается одна из наблюдаемых переменных  набора данных и относительно неё решается задача линейной регрессии оставшихся атрибутов. Итоговым ответом будет является усредненное значения показателя аномалии по всем атрибутам. 

Алгоритмы, основанные на линейном подходе, требуют  наличия линейной зависимости атрибутов данных. 
\subsection{Метрические методы}
Мерические методы пытаются найти в данных точки, в некотором смысле
изолированные от остальных\cite{Book01}. Если в пространстве задана некоторая метрика \textit{p(x1, x2)}, то необходимо задать следующие понятия:
\begin{itemize}
	\item  Аномалии – точки, не попадающие ни в один кластер. К данным применяется один из алгоритмов кластеризации; размер кластера, в котором оказалась точка, объявляется её показатель аномальности.
	\item Локальная плотность в аномальных точках низкая. Для данной точки показателем аномальности объявляется локальная плотность, которая оценивается некоторым непараметрическим способом.
	\item Расстояние от данной точки до ближайших соседей велико.
\end{itemize}
 В качестве показателя аномальности может выступать:
 \begin{itemize}
\item расстояние до k-го ближайшего соседа;
\item среднее расстояние до k ближайших соседей;
\item медиана расстояний до k ближайших соседей;
\item гармоническое среднее до k ближайших соседей;
\item доля из k ближайших соседей, для которых данная точка является не
более чем k-ым соседом и многое другое.
\end{itemize}
\subsubsection{Базовые понятия}
Метрические методы используют в случае отсутсвия априорной информации о данных. Сложность вычисления прямо пропорциональна как размерности данных m, так и их количеству n. При росте набора данных наблюдается экспоненциальный рост сложности вычислений. Однако, эти методы хорошо проявляют себя на ограниченных наборах данных\cite{Book07}. Следовательно такие методы как k-ближайших соседей(так же известный как обучение на основе примеров, и описанный поздее) с нотацией ассимптотического роста $O(n^2)$ недопустимы для наборов данных с большой размерностью, если их размерность не может быть уменьшена.

Существуют много  различных вариации алгоритма k-ближайших соседей для обнаружения аномалий, но все они основаны на вычислении некой метрики "расстояния до соседей"\ , такой как Евклидово расстояние или  расстояние Махаланобиса. Евклидово расстояние задается следующей формулой:
 	\begin{equation}
 	\sqrt{\sum_{i=1}^n(x_i-y_i)^2}
 		\end{equation}
 и является просто расстоянием между двумя точками, когда как  расстояние Махаланобиса, задаваемое следующей формулой
 	\begin{equation}
 	\sqrt{(x-\mu)^T C^-1 (x-\mu)}
 	\end{equation}
 	вычисляет расстояние от точки до центра тяжести ($\mu$), определяемого формулой коррелированных атрибутов, заданных матрицей ковариации (C). Расстояние  Махаланобиса
 	рассчитывается значительно дольше по сравнению с евлидковым
 	  для больших объемов данных, поскольку оно требует
 	обойти через весь набор данных, чтобы идентифицировать корреляции атрибутов.

 \subsubsection{K ближайших соседей(kNN)}
 Алгоритм работы метода:
 \begin{itemize}
 	\item Выбирается число K - число соседей.
 	\item Устанавилается граница показателя аномальности, на основе которой будет определятся метка элемента(задается в процентах относительно среднего показателя расстояния)
 	\item На основе метрики, рассчитывающей расстояния между элементами,  рассчитывается	 расстояние между всеми элементами и всеми его соседями.
 	\item Полученный результат сортируется на 
 	\item На основе полученных расстояний и границы показателя аномальности элементам присваиваются метки.
 \end{itemize}
 В качестве метрики, рассчитывающей расстояние между элементами можно использовать следующую формулу:
 \begin{equation}
 L=\sum_{0}^{k}x_{0} - x_{j}
 \end{equation}
 где $x_{j}$ - значение j-того атрибута элемента до которого ищется расстояние, а $x_0$-искомый элемент.

 \subsubsection{Оптимизация Рамасвани} 	
 Точка p является выбросом,
если не более n - 1 других точек в наборе данных имеют более высокий $D_m$(расстояние до m соседей), где m задается. Например на рисунке \ref{fig02} черная точка(обозначена звездочкой) является наиболее удаленной от соседей, следовательно она является выбросом. Красные точки(обозначены треугольниками) расположены рядом друг с другом, однако расстояние до других точек велико, следовательно они тоже являются аномалиями. Такой подход воcприимчив к вычислительному росту, потому что должна быть вычислена матрица расстояний точек друг от друга, поэтому Рамасвани в 2000 году предложил оптимизацию метода k-ближайших соседей  в виде составления ранжированного списка потенциальных выбросов.
\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{img/2_1.png}
	\caption{Пример поиска аномалий для метода k-ближайших соседей в двумерном пространстве}
	\label{fig02}
\end{figure}

 Оптимизация Рамасвани заключается в разбиении данных на ячейки.Если какая-либо ячейка и ее ближайшие соседи содержат больше, чем k
 точек, то точки в ячейке считаются лежащими в плотной области,
 поэтому содержащиеся там точки вряд ли будут выбросами. Если же почти все ячейки содержат больше,чем k точек, а какие-то ячейки содержат меньше, чем k точек, то тогда все точки, лежащие в ячейках, которые содержат  менее k элементов, помечаются  аномальным. Следовательно,
 необходимо обработать только небольшое количество ячеек, которые ранее не были помечены, и только относительно небольшое количество расстояний необходимо вычислить для обнаружений аномалий. 

 \subsubsection{Методы Кнора-Реймонда и Байерса-Рейтери}
 Кнор и Реймонд предложили свой эффективный метод  КНН подхода обучения без учителя\cite{Book09}. Если m из k ближайших соседей (где m<k) лежат
 в пределах определенного порогового значения d, тогда  считается, что  данные точки лежат в достаточно плотной области распределения данных, подлежащей классификации и подлежат классификации как нормальные, в противном случае они помечаются как аномальные.
 
  Очень похожий метод был придуман для идентефикации  наземных мин  на спутниковых снимках поверхности Земли Байеросом с соавторстве с Рейтери\cite{Book10}(этот метод можно использовать и для других целей). Он заключается в том, что берется m точек, для них ищется расстояние $D_m$. Если расстояние меньшего некого порогового значеня d,тогда  считается, что  данные точки лежат в достаточно плотной области распределения данных, подлежащей классификации и подлежат классификации как нормальные, в противном случае они помечаются как аномальные. Этот метод уменьшается количество варьируемых параметров, по сравнению с методом Кнора-Реймонда: остаются только параметры d и m, параметр k убирается. 
  подход оригинальный подход k-NN, поскольку только k ближайших соседей должны быть вычислены для каждой точки, а не всей матрицы расстояния
  для всех точек
\subsubsection{Метод Танга}
Метод Танга заключается  в вычислении средней цепочки расстояний между точкой p и k её соседями. Начальным расстояниям присваиваются более высокие веса, поэтому, если точка находится в разреженной
области как черная точка на рисунке \ref{fig02}, то путь до  ее ближайших соседей  будет относительно далеким, а среднее расстояние цепочки
будет высоким. Этот метод выгодно отлчичается от вышеописанных тем, что учитывает как  плотность, так и изоляцию. Рассмотрим рисунок \ref{fig03}.
Очевидно, что черные точки(обозначены звездочками) являются аномалиями, а скопление зеленых точек - множеством "нормальных"\ точек. Однако, алгоритмы k-NN классификации могут столкнутся с проблемой того, что расстояние от черных точек до зеленого кластера примерно равно, значит эти точки можно отнести к одной группе и при определенных значениях параметров алгоритма эти точки не будут считаться аномалиями. Метод Танга поможет избежать таких ошибок при обнаружении выбросов.
\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{img/3.png}
	\caption{Пример поиска аномалий методом Танга в двумерном пространстве}
	\label{fig03}
\end{figure}
 Однако метод является вычислительно сложным с временем выполнения как у оринального k-NN, поскольку он полагается на вычисление путей между всеми точками и их k соседей.
 \subsubsection{Метод ОДИН(ODIN)}
 Существуют модикации метода KNN, основанные на использовании графов. Рассмотрим один из них. Предположим, что у каждого элемента было найдено K ближайших соседей(используя евклидово расстояние). Тогда можно построить взвешенный ориентрированный граф, где  элемент представляет собой вершину, соединеную с K своими соседями ребрами\cite{Book19}. Весом ребра будет евклидово расстояние между двумя точками. При построении такого графа можно столкнуться с проблемой известной как "проблема всех к-соседей". Проблема состоит в том, что построение такого графа  может сопровождаться большой вычислительной сложностью. С решением этой проблемы можно ознакомится в \cite{Book20}.
 
 После построения графа начинается поиск аномалий. Аномалиями считаются все такие вершины, которые имеют 0 входящих ребер, а так же "замкнутые"\ группы элементов(такая группа элементов, которая имеет ребра только внутри себя, но не имеет связей с окружающими элементами).
 \subsubsection{Локальный коэффициент выбросов(LOF)}
 Этот метод является одним из самых известных алгоритмов обнаружения локальных аномалий. Недостатком метрических методов является тот факт, что все лежащие в их основе предположения верны лишь в дополнении друг с другом: локальная плотность точки, лежащей в центре небольшого кластера аномалий, может оказаться выше, чем для любой точки из большого кластера
 нормальных данных. Возможно и обратное: изолированная точка-аномалия может располагаться, например, в центре масс кластера нормальных точек, и тогда среднее расстояние от неё до соседей будет меньше, чем для нормальных точек. Это "свойство"\ метрических алгоритмов пытается учесть алгоритм  локального коэффициентов выбросов(англ.Local Outlier Factor).
 
 Чтобы вычислить LOF необходимо произвести следующие действия:
 \begin{enumerate}
 	\item Для каждой записи найти всех соседей, расстояния до которых не превышает k. Их количество может быть больше, чем k.
 	\item Использую эти записи для каждой точки $N_k$, вычислить локальную плотность точки, основанную на локальной плотности достижимости(англ. local reachability density (LRD)):
 	\begingroup
 	\Large
 	\begin{equation}
 	LRD_k(x) = 1/(\frac{\sum_{o \in N_k(x)}d_k (x,o)}{|N_k (x)|})
 	\end{equation}
 	\endgroup
 	где \begingroup
 	\Large$d_(x,o)$ \endgroup расстояние достигаемости. За редким исключеним в качестве расстояния достигаемости используется евклидово расстояние \cite{Book12}
 	\item Вычисляем LOF путем сравнения LRD записи с LRD соседей.
 	\begingroup
 	\Large
 	\begin{equation}
 	LOF(x) = \frac{\sum_{o \in N_k(x)}\frac{LRD_k (o)}{LRD_k (x)}}{|N_k (x)|}
 	\end{equation}
 	\endgroup
 \end{enumerate}
 Таким образом LOF является отношением локальных плотностей.  Нормальные записи, плотности которых равны плотности их соседей, получают оценку около 1,0. Аномалии, которые имеют низкую локальную плотность, получат значительно более высокую оценку. Алгоритм полагаясь только на свою прямую окрестность, формирует  оценку - величину, основанную  только на k соседях. Конечно, глобальные аномалии также могут быть обнаружены, так как они  имеют низкую LRD,по сравнению со своими соседями. Важно отметить, что в задачах обнаружения аномалий, где локальные аномалии не представляют интереса, этот алгоритм будет генерировать множество ложных аномалий. Hастройка k имеет решающее значение для этого алгоритма.
 
 Авторы алгоритма LOF рекомендуют использовать  для вычисления k стратегию ансамблирования( алгоритм описан ниже). Берется интервал возможных значений k и с некоторым шагом для всех возможных значений k вычисляется показатели аномальности для каждого элемента выборки. Путем голосования определяется является ли эта ли эта точка аномалий. Однако, на практике такие рекомендации редко используют из-за их значительной вычислительной сложности.
 \subsubsection{Компонентный коэффициент выбросов(COF)}
 \begin{figure}[h!]
 	\centering
 	\includegraphics[width=.5\textwidth]{img/5_3rdpart.png}
 	\caption{Сравнение COF (сверху) с LOF (внизу) с использованием простого набора данных с линейной корреляцией двух атрибутов}
 	\label{fig05}
 \end{figure}
 Компонентный коэффициент выбросов аналогичен LOF, но оценка плотности для записей выполняется иным способм В LOF k-ближайших соседей выбирают на основе евклидова расстояния. Это косвенно предполагает, что данные распределяются сферическим образом вокруг экземпляра. Если это допущение нарушено, например, если функции имеют прямую линейную корреляцию, то оценка плотности неверна. COF исправляет этот недостаток и оценивает локальную плотность окрестности с использованием метода кратчайшего пути, называемого расстоянием цепочки. Расстояние цепочки находится при помощи алгоритмов поиска кратчайшего пути. Например, когда функции, очевидно, коррелированы, этот подход оценки плотности работает значительно лучше \cite{Book14}. 
 На рисунке 5 показан результат для LOF и COF в сравнении для простого двумерного набора данных, где атрибуты имеют линейную зависимость. Можно видеть, что оценка  плотности LOF не может обнаружить выброс, но COF удадается связать  нормальные между собой для оценки локальной плотности.
 \subsubsection{Метод вероятностей локальных выбросов(LoOP)}
 Автора метода вероятностей локальных выбросов утверждает, что метод LOF может давать нестабильные  результаты\cite{Book18}. Поэтому была предложена модификация метода LOF.
 
 Предположим, что D - множество из n объектов, d- функция расстояния, используемая для выделения выбросов. В целях улучшения стабильности результатов введеном новую метрику - вероятностное расстояние 
 $o \in D$ до  множества $S \in D$, обозначаемое pdist(o,S).
 Вероятностное расстояние обладает следующим свойством:
 \begin{equation}
 \forall s \in S : P[(d(o,s)<pdist(o,S)] \geq \varphi
 \end{equation}
 Если визуализировать это выражение, то можно представить, что сфера вокруг о с радиусом pdist покрывает любой элемент в множестве S c вероятностью $\varphi$. Вероятностное расстояние pdist(o,S) от o до S можно интерпретировать как статистическое распределение множества S. 
 Вероятностное растояние рассчитывается по следующей формуле:
 \begin{equation}
 PLOF_{\lambda,S}(o)=\frac{pdist(\lambda,o, S(o))}{E_{s \in S(0)[pdist(\lambda,o, S(s))]}}
 \end{equation}
 Где E- центроида.
 Итоговая формула для расчёта LoOF выглядит следующим образом:
 \begin{equation}
 nPLOF=\lambda\sqrt{E[PLOF^2]}
 \end{equation}

  \begin{equation}
  LoOP_s(o)=max\{0,erf(\frac{PLOF_{\lambda,S}(o))}{nPLOF*\sqrt(2)})\}
  \end{equation}
 Где erf- Гауссова функция ошибок.
 С подробным выводом этих формул можно ознакомиться в \cite{Book18}.
\subsection{Параметрические методы}
\begin{figure}[!h]
	\centering
	\includegraphics[width=.5\textwidth]{img/4_1.png}
	\caption{Двухмерная проекция эллипсоиды минимального объема}
	\label{fig04}
\end{figure}
Вышеописанные методы не подходят для работы с большим объемом данных из-за их высокой временной сложности.
Параметрические методы позволяют очень быстро пересчитывать модель для
новых данных и подходит для больших наборов данных; модель растет
только с сложностью модели, а не размером данных. Однако они ограничивают
применимость,  применяя предварительно выбранную модель распределения для проверки данных на аномальность. Т.е. предварительно априорно подбирается модель правдоподобности данных. Элементы , которые значительно отклоняются от этой модели считаются аномальными. Параметрический подход схож с линейным по описанию, но значительно отличается от него по принципу работы.

Одним из таких подходов является оценка эллипсоидой минимального объема\cite{Book11}, которая соответствует наименьшему допустимому эллипсоиду, покрывающему не меньше 50\% точек выборки.



\section{Методы улучшения алгоритмов поиска аномалий}
Алгоритмы поиска аномалий можно улучшать различными методами, применяемыми в том числе для  улучшения результатов работы алгоритмов и в других областях. Например, ансамблирование широко применяется для работы с нейронными сетями. Ниже рассмотрены приемы в контексте поиска аномалий.
\subsection{Cемплирование}
Большинство алгоритмов распознавания аномалий успешно работают на наборах данных малых размеров. Поэтому предлагается разбить начальный набор данных на несколько случайных выборок и усреднить результат. Размер этих выборок может быть как и случайным, там и фиксированного размера, но, как правило, он отличается от размеров исходного набора данных не меньше чем на порядок. Идея такого выбора заключается в том, что шумовые объекты попадут в выборки с низкой вероятностью; кластера нормальных данных будут представлены несколькими представителями, а кластера аномалий выродятся в изолированные точки. На основе этих выборок алгоритмы строят функции показателя аномальности, незначительно уступующему результату, полученному на основе анализа всех исходных данных. 

Этот метод помогает значительно сократить вычислительную сложность, а так же уменьшить вероятность "подгона"\ алгоритма под конкретный набор данных.  В силу особенностей задачи, необходимое условие - отсутствия параметризации алгоритмов -  зачастую означает их детерминированность(в отсутсвии стохастичности, показатель аномальности однозначно определяется по  заданной выборке). В общем случае при добавлении новых данных в общий набор данных, можно не пересчитывать заново показатель аномальности для всего набора данных, а добавить запуски алгоритма на новых данных в ансамбль(так называемый warm start\cite{Book15})
\subsection{Ансамблирование голосованием}
Ансамблированием в задаче поиска аномалий называют использование нескольких различных алгоритмов с последующи усреднением их показателя аномальности. При использовании различных классов алгоритмов можно столкнуться с проблемой того, что показатель аномальности выглядит по-разному в различных алгоритмах и сравнивать напрямую эти показатели некорректно.  Поэтому традиционное приведение  показателей значений различных функций к одному диапазону, например,к [0,1], будет некорректным.
Существует несколько наиболее известных видов ансамблирования:
\begin{itemize}
	\item Простое голосование 
	\begin{equation}
	b(x)=F(b_1(x),...,b_T(x))=\frac{1}{T}\sum_{t=1}^{T}b_t(x)
	\end{equation}
	,где $b_i$ -  некоторая функция.
	\item Взвешенное голосование 
	\begin{gather}
	b(x)=F(b_1(x),...,b_T(x))=\frac{1}{T}\sum_{t=1}^{T}w_tb_t(x)\\
		\sum_{t=1}^{T}w_t=1, w_t \geq 0
	\end{gather}
	,где $w_i$- некоторый коэффициент.
		\item Cмесь экспертов
		\begin{gather}
		b(x)=F(b_1(x),...,b_T(x))=\frac{1}{T}\sum_{t=1}^{T}w_t(x)b_t(x)\\
		\sum_{t=1}^{T}w_t=1, \forall x\in X
		\end{gather}
		,где $w_i(x)$- некоторая функция коэффициента.
 \end{itemize}
	Простое голосование - это  частный случай взвешенного голосования, а взвешенное голосование является частным случаем смеси экспертов. 
	
	Различные методы ансамблирования такие как беггинг, бустинг, стекинг и другие применяются для улучшения работы алгоритмов обучения с учителем . Для алгоритмов обучения без учителя применяется простое голосование, т.к. задача изменения весов голосования нетривиальна в задаче обучения без учителя\cite{Book16}.
\subsection{Итеративный отбор}
Итеративный отбор основан на идее многократного применения алгоритмов ансамблирования. Преположим, построена некоторая модель, описывающая нормальные данные. Эта модель построена на основе всех  имеющихся данных, но точность этой модели невелика, она умеет определять только явные аномалии. Отсортировав все точки по показателю аномальности, можно выбрать k самых аномальных объекта в данных и исключить из данных. После этого можно перестроить модель и повторить вышеуказанные действия несколько раз, пока не будут достигнуты некоторые условия. При каждой итерации точность модели будет увеличиваться.

Идея итеративного отбора может быть обобщена различными способами. Результат работы одного алгоритма может быть использован для отсеивания явных аномалий и настройки нового алгоритма, не обязательно совпадающего с предыдущим, на оставшихся данных. Возможна и противоположная механика: по результатам работы одного алгоритма отбираются явные, гарантированные
представители нормальных данных, и исключительно на них строится модель, их описывающая.
\section{Выводы}

\begin{table}[!h]
	
	\caption{\label{tab:collectdata}Сравнение алгоритмов поиска аномалий}
	
	\begin{center}
		
		\begin{tabular}{|l|l|l|l|}
			
			\hline
			
			Класс методов & Временная сложность & Расход памяти & Универсальность \\
			
			\hline \hline
			
			Вероятностно-ген.& O(1) &  O(n) & Очень низкая \\
			
			
			\hline
			
			Линейный &  $\ge O(n^2)$ &  $\ge O(n^2)$ & Низкая\\
			
			
			\hline
			Параметрический & O(1) & O(n) & Низкая\\
			\hline
			Метрический & $\ge O(nlogn)$ & $\ge O(n)$ & Высокая\\
			
			
			\hline
			
			
		\end{tabular}
		
	\end{center}
	
\end{table}                               
Под универсальностью понимается возможность применять алгоритмы к различным набором данных, не обладающих специфическими характеристиками, и, не обладая априорной информацией, получать высокую точность классификации.

Существует большое число алгоритмов для нахождения аномалий. Некоторые из них опирается на априорные данные, некоторые не опираются. Для выбора подходящего алгоритма нахождения аномалий зачастую стоит учитывать характер данных, их размер и доступную априорую информацию. Несмотря на то, область знаний обнаружения аномалий активно развивается как часть современной науки, остается ещё много простора для исследования алгоритмов, модификации и создания новых.


